{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Args:\n",
    "    input_size: The number of expected features in the input `x`\n",
    "    hidden_size: The number of features in the hidden state `h`\n",
    "    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
    "        would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
    "        with the second LSTM taking in outputs of the first LSTM and\n",
    "        computing the final results. Default: 1\n",
    "    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
    "        Default: ``True``\n",
    "    batch_first: If ``True``, then the input and output tensors are provided\n",
    "        as `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n",
    "        Note that this does not apply to hidden or cell states. See the\n",
    "        Inputs/Outputs sections below for details.  Default: ``False``\n",
    "    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
    "        LSTM layer except the last layer, with dropout probability equal to\n",
    "        :attr:`dropout`. Default: 0\n",
    "    bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``\n",
    "    proj_size: If ``> 0``, will use LSTM with projections of corresponding size. Default: 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inputs: input, (h_0, c_0)\n",
    "    * **input**: tensor of shape :math:`(L, N, H_{in})` when ``batch_first=False`` or\n",
    "      :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of\n",
    "      the input sequence.  The input can also be a packed variable length sequence.\n",
    "      See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
    "      :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
    "    * **h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n",
    "      initial hidden state for each element in the batch.\n",
    "      Defaults to zeros if (h_0, c_0) is not provided.\n",
    "    * **c_0**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n",
    "      initial cell state for each element in the batch.\n",
    "      Defaults to zeros if (h_0, c_0) is not provided.\n",
    "    where:\n",
    "    .. math::\n",
    "        $$\n",
    "        \\begin{aligned}\n",
    "            N ={} & \\text{batch size} \\\\\n",
    "            L ={} & \\text{sequence length} \\\\\n",
    "            D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n",
    "            H_{in} ={} & \\text{input\\_size} \\\\\n",
    "            H_{cell} ={} & \\text{hidden\\_size} \\\\\n",
    "            H_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\\n",
    "        \\end{aligned}\n",
    "        $$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Outputs: output, (h_n, c_n)\n",
    "    * **output**: tensor of shape :math:`(L, N, D * H_{out})` when ``batch_first=False`` or\n",
    "      :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features\n",
    "      `(h_t)` from the last layer of the LSTM, for each `t`. If a\n",
    "      :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output\n",
    "      will also be a packed sequence.\n",
    "    * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n",
    "      final hidden state for each element in the batch.\n",
    "    * **c_n**: tensor of shape :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n",
    "      final cell state for each element in the batch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "rnn = nn.LSTM(input_size=32, hidden_size=20, num_layers=2, batch_first=True, bidirectional=True)\n",
    "# (L, N, H_{in}) when batch_first=False\n",
    "# (N, L, H_{in}) when batch_first=True containing the features of the input sequence.\n",
    "input = torch.randn(3, 81, 32)\n",
    "# (D * \\text{num\\_layers}, N, H_{out})\n",
    "# h0 = torch.randn(2, 3, 20)\n",
    "# c0 = torch.randn(2, 3, 20)\n",
    "# output, (hn, cn) = rnn(input, (h0, c0))\n",
    "output, (hn, cn) = rnn(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}